{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "COLOR_TREAT = \"#2ecc71\"\n",
    "COLOR_NO_TREAT = \"#e74c3c\"\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalonde = pd.read_csv('lalonde.csv')\n",
    "#lalonde.set_index('id', drop=True, inplace=True)\n",
    "lalonde.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Naive analysis\n",
    "\n",
    "TODO make a plot more comparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_distrib(s1, s2, title, xLabel, yLabel, ax=None):\n",
    "    bins = np.histogram(s1)[1]\n",
    "    sns.distplot(s1, kde=False, color=COLOR_NO_TREAT, norm_hist=True, ax=ax, bins=bins)\n",
    "    sns.distplot(s2, kde=False, color=COLOR_TREAT, norm_hist=True, ax=ax, bins=bins)\n",
    "    if ax is None:\n",
    "        plt.title(title)\n",
    "        plt.xlabel(xLabel)\n",
    "        plt.ylabel(yLabel)\n",
    "        plt.legend(['No treatment', 'Treatment'])\n",
    "    else:\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(xLabel)\n",
    "        ax.set_ylabel(yLabel)\n",
    "        ax.legend(['No treatment', 'Treatment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plot_distrib(s1=lalonde.re78[lalonde['treat'] == 0], s2=lalonde.re78[lalonde['treat'] == 1], title='Distribution of the revenues', ax=None, xLabel='Revenue in 1978', yLabel='Number of persons (Density)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the average earnings of people that have/have not participated the job training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "axes = plt.subplot(121)\n",
    "lalonde.groupby(['treat'])['re78'].mean().plot.bar(color=[COLOR_NO_TREAT, COLOR_TREAT])\n",
    "plt.title('Mean salary with/without training')\n",
    "plt.ylabel('Mean salaray')\n",
    "plt.xlabel('Traning')\n",
    "plt.subplot(122)\n",
    "lalonde.groupby(['treat'])['re78'].median().plot.bar(color=[COLOR_NO_TREAT, COLOR_TREAT])\n",
    "plt.title('Median salary with/without training')\n",
    "plt.ylabel('Median Salary')\n",
    "plt.xlabel('Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A closer look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distinguish categorical from non-categorical features\n",
    "sns.set(font_scale=1.2)\n",
    "lalonde_cat = lalonde[['black', 'hispan', 'married', 'nodegree', 'treat']]\n",
    "lalonde_non_cat = lalonde[['age', 'educ', 're74', 're75', 're78', 'treat']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each feature, compare its distribution in the treated group with its distribution in the control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(3, 3, figsize=(15, 18))\n",
    "for index, feature in enumerate(['age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75', 're78']):\n",
    "    ax = axarr[int(index/3)][index%3]\n",
    "    plot_distrib(s1=lalonde[feature][lalonde['treat'] == 0], s2=lalonde[feature][lalonde['treat'] == 1], title=feature, xLabel = feature, yLabel='Density', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise analysis of features' distribution\n",
    "Comparing the distrbution of features for each distinct feature tuple can give additional information:\n",
    "**TODO : décrire les obs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(lalonde_non_cat, hue='treat', palette={0:\"#e74c3c\", 1: \"#2ecc71\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A propsensity score model\n",
    "We should fit our model on the pre treatment features, though we will have to remove the re78 feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "lal = lalonde.drop(['id','treat','re78'],1)\n",
    "lal = preprocessing.scale(lal)\n",
    "model = sklearn.linear_model.LogisticRegression()\n",
    "model.fit(lal, lalonde.treat)\n",
    "pred = model.predict_proba(lal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(model.predict(lal) == lalonde.treat)/len(lal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalonde['pred'] = pred[:,1]\n",
    "ax = sns.stripplot(x='id', y='pred', hue='treat', data=lalonde, palette={0:\"#e74c3c\", 1: \"#2ecc71\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Balancing the dataset via matching\n",
    "Use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group. (Hint: you may explore the networkx package in Python for predefined matching functions.)\n",
    "\n",
    "Your matching should maximize the similarity between matched subjects, as captured by their propensity scores. In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "After matching, you have as many treated as you have control subjects. Compare the outcomes (re78) between the two groups (treated and control).\n",
    "\n",
    "Also, compare again the feature-value distributions between the two groups, as you've done in part 2 above, but now only for the matched subjects. What do you observe? Are you closer to being able to draw valid conclusions now than you were before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "#graph_size = lalonde['treat'].value_counts()\n",
    "#G = nx.complete_bipartite_graph(graph_size[0], graph_size[1])\n",
    "#G.add_nodes_from(lalonde['id'][lalonde.treat == 0], bipartite=0) # Add the node attribute \"bipartite\"\n",
    "#G.add_nodes_from(lalonde['id'][lalonde.treat == 1], bipartite=1)\n",
    "\n",
    "G=nx.Graph()\n",
    "G.add_nodes_from(lalonde['id'][lalonde.treat == 0]) # Add the node attribute \"bipartite\"\n",
    "G.add_nodes_from(lalonde['id'][lalonde.treat == 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ID_u, score_u in zip(lalonde.id[lalonde.treat == 0], lalonde.pred[lalonde.treat == 0]):\n",
    "    for ID_v, score_v in zip(lalonde.id[lalonde.treat == 1], lalonde.pred[lalonde.treat == 1]):\n",
    "        G.add_edge(ID_u, ID_v, weight=-abs(score_u-score_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G.number_of_nodes()\n",
    "G.number_of_edges()\n",
    "#nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from networkx.algorithms import max_weight_matching\n",
    "matching = max_weight_matching(G, maxcardinality=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dict()\n",
    "for key in matching.keys():\n",
    "    if(key[0] == 'N'):\n",
    "        res[key] = matching[key]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalonde_treated = lalonde[lalonde['treat'] == 1]\n",
    "lalonde_treated['temp'] = 1\n",
    "#lalonde_treated\n",
    "\n",
    "lalonde_nt = lalonde[lalonde['treat'] == 0]\n",
    "lalonde_nt['temp'] = 1\n",
    "#lalonde_nt\n",
    "\n",
    "result = pd.merge(lalonde_treated, lalonde_nt, on='temp')\n",
    "result = result[['id_x' , 'id_y' , 'pred_x' , 'pred_y']]\n",
    "result['diff'] = abs(result['pred_x'] - result['pred_y'])\n",
    "result = result.set_index(['id_x', 'id_y'])\n",
    "sum(result.loc[list(res.items())]['diff'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[list(res.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lalonde.set_index('id' , inplace = True)\n",
    "matched = lalonde.loc[list(matching.keys())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = lalonde.loc[list(matching.keys())]\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(121)\n",
    "plt.title('Mean salary with/without training')\n",
    "plt.ylabel('Mean salaray')\n",
    "matched.groupby(['treat'])['re78'].mean().plot.bar(color=[COLOR_NO_TREAT, COLOR_TREAT])\n",
    "plt.subplot(122)\n",
    "plt.title('Median salary with/without training')\n",
    "plt.ylabel('Median Salary')\n",
    "matched.groupby(['treat'])['re78'].median().plot.bar(color=[COLOR_NO_TREAT, COLOR_TREAT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distrib(s1=matched.re78[matched['treat'] == 0], s2=matched.re78[matched['treat'] == 1], \n",
    "             title='Distribution of the revenues', ax=None, xLabel='Revenue in 1978', yLabel='Number of persons (Density)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(3, 3, figsize=(15, 18))\n",
    "for index, feature in enumerate(['age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75', 're78']):\n",
    "    ax = axarr[int(index/3)][index%3]\n",
    "    plot_distrib(s1=matched[feature][matched['treat'] == 0], s2=matched[feature][matched['treat'] == 1], title=feature, xLabel = feature, yLabel='Density', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "for feature in ['age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75', 're78']:\n",
    "    ks = stats.ks_2samp(matched[feature][matched['treat'] == 0], matched[feature][matched['treat'] == 1])\n",
    "    print('For ' + str(feature)+ ' the KS stat and P-values are ' +str(ks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Balancing the groups further\n",
    "The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalonde = pd.read_csv('lalonde.csv')\n",
    "lal = lalonde.drop(['id','treat','re78'],1)\n",
    "lal = preprocessing.scale(lal)\n",
    "model = sklearn.linear_model.LogisticRegression()\n",
    "model.fit(lal, lalonde.treat)\n",
    "pred = model.predict_proba(lal)\n",
    "lalonde['pred'] = pred[:,1]\n",
    "\n",
    "\n",
    "Gprime=nx.Graph()\n",
    "Gprime.add_nodes_from(lalonde['id'][lalonde.treat == 0])\n",
    "Gprime.add_nodes_from(lalonde['id'][lalonde.treat == 1])\n",
    "\n",
    "lalonde_not_black = lalonde[lalonde.black == 0]\n",
    "for ID_u, score_u in zip(lalonde_not_black.id[lalonde_not_black.treat == 0], lalonde_not_black.pred[lalonde_not_black.treat == 0]):\n",
    "    for ID_v, score_v in zip(lalonde_not_black.id[lalonde_not_black.treat == 1], lalonde_not_black.pred[lalonde_not_black.treat == 1]):\n",
    "        Gprime.add_edge(ID_u, ID_v, weight=-abs(score_u-score_v))\n",
    "\n",
    "lalonde_black = lalonde[lalonde.black == 1]\n",
    "for ID_u, score_u in zip(lalonde_black.id[lalonde_black.treat == 0], lalonde_black.pred[lalonde_black.treat == 0]):\n",
    "    for ID_v, score_v in zip(lalonde_black.id[lalonde_black.treat == 1], lalonde_black.pred[lalonde_black.treat == 1]):\n",
    "        Gprime.add_edge(ID_u, ID_v, weight=-abs(score_u-score_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching = max_weight_matching(Gprime, maxcardinality=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalonde.set_index('id' , inplace = True)\n",
    "matched = lalonde.loc[list(matching.keys())]\n",
    "\n",
    "f, axarr = plt.subplots(3, 3, figsize=(15, 18))\n",
    "for index, feature in enumerate(['age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75', 're78']):\n",
    "    ax = axarr[int(index/3)][index%3]\n",
    "    plot_distrib(s1=matched[feature][matched['treat'] == 0], s2=matched[feature][matched['treat'] == 1], title=feature, xLabel = feature, yLabel='Density', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "for feature in ['age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75', 're78']:\n",
    "    ks = stats.ks_2samp(matched[feature][matched['treat'] == 0], matched[feature][matched['treat'] == 1])\n",
    "    print('For ' + str(feature)+ ' the KS stat and P-values are ' +str(ks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. A less naive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "axes = plt.subplot(121)\n",
    "matched.groupby(['treat'])['re78'].mean().plot.bar(color=[COLOR_NO_TREAT, COLOR_TREAT])\n",
    "plt.title('Mean salary with/without training')\n",
    "plt.ylabel('Mean salaray')\n",
    "plt.xlabel('Traning')\n",
    "plt.subplot(122)\n",
    "matched.groupby(['treat'])['re78'].median().plot.bar(color=[COLOR_NO_TREAT, COLOR_TREAT])\n",
    "plt.title('Median salary with/without training')\n",
    "plt.ylabel('Median Salary')\n",
    "plt.xlabel('Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Applied ML\n",
    "We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project if you plan to work with text!\n",
    "\n",
    "- Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn. TF-IDF, short for term frequency–inverse document frequency, is of great help when if comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using TfidfVectorizer. Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category).\n",
    "\n",
    "- Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the feature_importances_ attribute of your random forest and discuss the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all the target names inside the newsgroup dataset\n",
    "## 2.1 Load the data, vectorize it and split it into training set and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(newsgroups.data) \n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has 18846 datapoints and 173762 features. Now let split it into train, valid and test set. To do so, we created a function *get_train_valid_test_set* written in the box below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_valid_test_set(data, labels, perc_for_valid, perc_for_test):\n",
    "    # returns the train, valid and test sets\n",
    "    # data : n x nb_features matrix\n",
    "    # perc_for_valid : value between 0 and 1 as the percentage of the dataset to be used in validation\n",
    "    # perc_for_test : value between 0 and 1 as the percentage of the dataset to be used in testing\n",
    "    \n",
    "    uindx = np.random.permutation(np.shape(data)[0])\n",
    "    X_shuffled = data[uindx]\n",
    "    labels_shf = labels[uindx]\n",
    "    \n",
    "    # take perc_for_test% of the dataset for testing\n",
    "    X_test_shf = X_shuffled[:int(X_shuffled.shape[0]*perc_for_test)]\n",
    "    y_test_shf = labels_shf[:int(X_shuffled.shape[0]*perc_for_test)]\n",
    "    \n",
    "    # take perc_for_valid% of the dataset for validation\n",
    "    X_valid_shf = X_shuffled[int(X_shuffled.shape[0]*perc_for_test):\n",
    "                             int(X_shuffled.shape[0]*(perc_for_test + perc_for_valid))]\n",
    "    y_valid_shf = labels_shf[int(X_shuffled.shape[0]*perc_for_test):\n",
    "                             int(X_shuffled.shape[0]*(perc_for_test + perc_for_valid))]\n",
    "    \n",
    "    X_train_shf = X_shuffled[int(X_shuffled.shape[0]*(perc_for_test + perc_for_valid)):]\n",
    "    y_train_shf = labels_shf[int(X_shuffled.shape[0]*(perc_for_test + perc_for_valid)):]\n",
    "    \n",
    "    return X_test_shf, X_train_shf, X_valid_shf, y_test_shf, y_train_shf, y_valid_shf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, X_train, X_valid, y_test, y_train, y_valid = get_train_valid_test_set(X, newsgroups.target, 0.1, 0.1)\n",
    "print('test set size : '+ str(X_test.shape))\n",
    "print('valid set size : '+ str(X_valid.shape))\n",
    "print('train set size : '+ str(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Classification\n",
    "**Train a random forest** on your training set. Try to **fine-tune the parameters** of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, **display a confusion matrix** of your classification pipeline. Lastly, once you assessed your model, **inspect the feature_importances_** attribute of your random forest and discuss the obtained results.\n",
    "\n",
    "### Fine Tune max depth and number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "number_trees = [125, 300, 500, 600 ]\n",
    "max_depth = [5, 8, 12, 16, 20]\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "#Loop for hyperparameter number_trees and max_depth\n",
    "for nb_t in number_trees:\n",
    "    for nb_d in max_depth:\n",
    "\n",
    "        # Random forest model\n",
    "        rand_forest_model = RandomForestClassifier(n_estimators=nb_t, max_depth=nb_d)\n",
    "        rand_forest_model.fit(X_train, y_train)\n",
    "        y_pred_val = rand_forest_model.predict(X_valid)\n",
    "\n",
    "        # get score\n",
    "        score = accuracy_score(y_valid, y_pred_val)\n",
    "        print('loading : '+ str(stepinfo/(len(number_trees)*len(max_depth))) + '% nb_trees : ' + str(nb_t) + ' depth : '\n",
    "              + str(nb_d) + ' score : ' + str(score))\n",
    "        \n",
    "        # update best score if needed\n",
    "        if score > best_score:\n",
    "            best_nb_trees = nb_t\n",
    "            best_max_depth = nb_d\n",
    "            best_score = score\n",
    "print(best_score)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_forest_model = RandomForestClassifier(n_estimators=best_nb_trees, max_depth=best_max_depth)\n",
    "rand_forest_model.fit(X_train, y_train)\n",
    "y_pred_test = rand_forest_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred_test, y_test)\n",
    "#np.fill_diagonal(cm, 0)\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO : regarder comment mettre la confusion matrix en % \n",
    "Faire la features importance\n",
    "Analayser tout ca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
